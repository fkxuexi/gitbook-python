### 1、线性回归可以做什么：

- 可以进行预测：

    利用这些分算的点，在结合一元线性回归可以拟合出一条直线，这样我们就可以通过这个直线方程来预测出后面的走势，如下图我们可以带入我们需要预测的自变量，从而求出因变量

- 因子分析：

    因子分析是根据回归分析结果，得出各个自变量对目标变量产生的影响，因此，需要求出各个自变量的影响程度。 

![](/assets/LinearRegression.png)

### 2、方法及相关术语：

- #### 1、最小平方法(最小二乘法)
![](/assets/1464445402.png)
  y = kx + b ; 在直线上的点我们称之为预测点，离散的点我们称之为实际点
  
  回归线则使得，实际点与预测点之差平方和最小(之所以是使用平方，是防止有负值相互抵消)
  
  S=$$\sum_{i=1}^N(y-(ax+b))^2$$  
  
 在这里我们是需要求出a和b的值来使得 S 最小，故此这个函数就是关于a和b的函数，要想求的最小值，我们就必须使用求导啦，对于一元方程我们只需要求导即可，对于高阶方程就需要求偏导数了， 总之再有多个点的情况下我们是可以算出a和b的值的
 
### 3、评价回归线拟合的好坏：

- #### 总偏差（总平方和 SSR(Sum of Squaresfor Total)）：     
  
  描述的是：(实际点-实际点的平均值)^2 
  
 SSR = $$\sum_{i=1}^N(y-Y)^2$$       
 
                                             
                                                                                         
                                                                                                                                     
                                                                                                                                                                                 
                                                                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                     